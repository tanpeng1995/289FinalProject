{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "import tifffile as tif\n",
    "from scipy import signal\n",
    "import cv2\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backprojection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(size, sigma=None):\n",
    "    if sigma == None:\n",
    "        sigma = size // 6\n",
    "    center = size // 2\n",
    "    kernel = np.zeros((size, size))\n",
    "    for i in np.arange(size):\n",
    "        for j in np.arange(size):\n",
    "            diff2 = (i-center)**2 + (j-center)**2\n",
    "            kernel[i,j] = np.exp(-diff2/sigma**2/2)\n",
    "    return kernel/np.sum(kernel)\n",
    "\n",
    "def backprojection(high_resolution_img, low_resolution_img, max_iters):\n",
    "    row_h, col_h = high_resolution_img.shape\n",
    "    row_l, col_l = low_resolution_img.shape\n",
    "    p = gaussian_kernel(5,1)\n",
    "    p = p**2\n",
    "    p = p/np.sum(p)\n",
    "\n",
    "    high_resolution_img = high_resolution_img.astype(float)\n",
    "    low_resolution_img  = low_resolution_img.astype(float)\n",
    "\n",
    "    for i in np.arange(max_iters):\n",
    "        temp = cv2.resize(high_resolution_img, (row_l, col_l), interpolation=cv2.INTER_CUBIC)\n",
    "        image_diff = low_resolution_img - temp\n",
    "\n",
    "        image_diff = cv2.resize(image_diff, (row_h, col_h), interpolation=cv2.INTER_CUBIC)\n",
    "        high_resolution_img = high_resolution_img + signal.convolve2d(image_diff, p, 'same')\n",
    "    return high_resolution_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_low_resolution_features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_low_resolution_features(low_resolution_img):\n",
    "    row, col = low_resolution_img.shape\n",
    "    features = np.zeros((row,col,4))\n",
    "    # first order gradient filters\n",
    "    hf1 = np.array([-1,0,1]).reshape(1,-1)\n",
    "    vf1 = hf1.T\n",
    "    features[:,:,0] = signal.convolve2d(low_resolution_img, hf1, 'same')\n",
    "    features[:,:,1] = signal.convolve2d(low_resolution_img, vf1, 'same')\n",
    "\n",
    "    # second order gradient filters\n",
    "    hf2 = np.array([1,0,-2,0,1]).reshape(1,-1)\n",
    "    vf2 = hf2.T\n",
    "    features[:,:,2] = signal.convolve2d(low_resolution_img, hf2, 'same')\n",
    "    features[:,:,3] = signal.convolve2d(low_resolution_img, vf2, 'same')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_compact_X.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compact_X(img_path, num_basis = 512, labda = 0.15, patch_size = 5, num_patches = 10000, upscale = 2, threshold = 10):\n",
    "    # randomly sample image patches\n",
    "    X_high, X_low = rnd_smp_patch(img_path, '*.tif', patch_size, num_patches, upscale)\n",
    "\n",
    "    # prune patches wiith small variances\n",
    "    # threshold chosen based on the training data\n",
    "    X_high, X_low = patch_pruning(X_high, X_low, threshold)\n",
    "\n",
    "    # train coupled sparse coding dictionary\n",
    "    # D_high, D_low = train_coupled_dict(X_high, X_low, num_basis, labda, upscale)\n",
    "    X = train_coupled_dict_before_opt(X_high, X_low, num_basis, labda, upscale)\n",
    "\n",
    "    # save data\n",
    "    save_path = 'Training/compact'+'_'+str(num_basis)+'_'\\\n",
    "        +str(labda)+'_'+str(upscale)+'_'\n",
    "    savetxt(save_path+'X', X, delimiter=' ')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img_super_resolution.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_scale(hPatch, mNorm):\n",
    "    hNorm = np.linalg.norm(hPatch)\n",
    "    if hNorm > 0:\n",
    "        scale = 1.2 * mNorm / hNorm\n",
    "        hPatch = scale * hPatch\n",
    "    return hPatch\n",
    "\n",
    "def normalize_image(img, L):\n",
    "    _min, _max = img.min(), img.max()\n",
    "    img = ((img-_min)/(_max-_min) * (L-1)).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def img_super_resolution(low_resolution_img, upscale, D_high, D_low, labda, overlap):\n",
    "    # normalize the dictionary\n",
    "    D_high = D_high/np.sqrt(np.sum(D_high**2, axis=0))\n",
    "    D_low  = D_low/np.sqrt(np.sum(D_low**2, axis=0))\n",
    "\n",
    "    # get patch_size\n",
    "    patch_size = int(np.sqrt(D_high.shape[0]))\n",
    "\n",
    "    # bicubic interpolation of the low_resolution_img\n",
    "    m, n = low_resolution_img.shape\n",
    "    medium_resolution_img = cv2.resize(low_resolution_img, \\\n",
    "        (m*upscale, n*upscale), interpolation=cv2.INTER_CUBIC)\n",
    "    M, N = medium_resolution_img.shape\n",
    "\n",
    "    # initialize high_resolution_img\n",
    "    high_resolution_img = np.zeros(medium_resolution_img.shape)\n",
    "    count_map = np.zeros(medium_resolution_img.shape)\n",
    "\n",
    "    # extract low_resolution_img features\n",
    "    features = extract_low_resolution_features(medium_resolution_img)\n",
    "\n",
    "    # patch index for sparse recovery, avoid boundary\n",
    "    p = patch_size//2\n",
    "    gridx = np.array(list(range(p,M-patch_size-p,patch_size-overlap))+[M-patch_size-p])\n",
    "    gridy = np.array(list(range(p,N-patch_size-p,patch_size-overlap))+[N-patch_size-p])\n",
    "\n",
    "    A = D_low.T @ D_low\n",
    "\n",
    "    # loop to recover each high resolution patch\n",
    "    for i in np.arange(len(gridx)):\n",
    "        for j in np.arange(len(gridx)):\n",
    "            patch_idx = i*len(gridy)+j\n",
    "            #print('current patch_idx is {}'.format(patch_idx))\n",
    "            xx = gridx[i]\n",
    "            yy = gridy[j]\n",
    "\n",
    "            # column feature\n",
    "            mPatch = medium_resolution_img[xx:xx+patch_size, yy:yy+patch_size].reshape(-1)\n",
    "            mMean  = np.mean(mPatch)\n",
    "            mPatch = mPatch - mMean\n",
    "            mNorm  = np.linalg.norm(mPatch)\n",
    "\n",
    "            mPatchFea = features[xx:xx+patch_size, yy:yy+patch_size, :].reshape(-1)\n",
    "            mPatchFea = mPatchFea - np.mean(mPatchFea)\n",
    "            mFeaNorm  = np.linalg.norm(mPatchFea)\n",
    "\n",
    "            y = mPatchFea / mNorm if mFeaNorm > 1 else mPatchFea\n",
    "            b = -D_low.T @ y\n",
    "\n",
    "            # solve for sparce coefficient using feature sign\n",
    "            w = L1_FeatureSign(labda, A, b)\n",
    "            #print(len(np.where(w!=0)[0]))\n",
    "\n",
    "            # recover high resolution patch and scale the contrast\n",
    "            hPatch = D_high @ w\n",
    "            hPatch = lin_scale(hPatch, mNorm)\n",
    "            hPatch = hPatch.reshape(patch_size, patch_size)\n",
    "            hPatch = hPatch + mMean\n",
    "\n",
    "            high_resolution_img[xx:xx+patch_size, yy:yy+patch_size] += hPatch\n",
    "            count_map[xx:xx+patch_size, yy:yy+patch_size] += 1\n",
    "\n",
    "    # fill in the empty with bicubic interpolation\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(count_map)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(high_resolution_img)\n",
    "    high_resolution_img[count_map < 1] = medium_resolution_img[count_map < 1]\n",
    "    count_map[count_map < 1 ] = 1\n",
    "    high_resolution_img = high_resolution_img / count_map\n",
    "    high_resolution_img = normalize_image(high_resolution_img, 256)\n",
    "\n",
    "    return high_resolution_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnd_smp_patch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(img, L):\n",
    "    _min, _max = img.min(), img.max()\n",
    "    img = ((img-_min)/(_max-_min) * (L-1)).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def sample_patches(img, patch_size, num_copies, upscale):\n",
    "    high_resolution_img = img\n",
    "    high_resolution_img = high_resolution_img.astype(float)\n",
    "    m, n = high_resolution_img.shape\n",
    "    # generate low resolution counterparts\n",
    "    low_resolution_img  = cv2.resize(\\\n",
    "        high_resolution_img, (m//upscale, n//upscale),interpolation=cv2.INTER_CUBIC)\n",
    "    low_resolution_img  = cv2.resize(\\\n",
    "        low_resolution_img, (m, n), interpolation=cv2.INTER_CUBIC)\n",
    "    low_resolution_img = low_resolution_img.astype(float)\n",
    "\n",
    "    x = np.arange(m-2*patch_size)+patch_size\n",
    "    y = np.arange(n-2*patch_size)+patch_size\n",
    "    np.random.shuffle(x)\n",
    "    np.random.shuffle(y)\n",
    "\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    xrow, ycol = X.reshape(-1), Y.reshape(-1)\n",
    "    if num_copies < len(xrow):\n",
    "        xrow = xrow[:num_copies]\n",
    "        ycol = ycol[:num_copies]\n",
    "    else:\n",
    "        num_copies = len(xrow)\n",
    "\n",
    "    # initialize output\n",
    "    x_high = np.zeros((patch_size**2, num_copies))\n",
    "    x_low  = np.zeros((4*patch_size**2, num_copies))\n",
    "\n",
    "    # compute the first and second order gradients\n",
    "    hf1 = np.array([-1,0,1]).reshape(1,-1)\n",
    "    vf1 = hf1.T\n",
    "    hf2 = np.array([1,0,-2,0,1]).reshape(1,-1)\n",
    "    vf2 = hf2.T\n",
    "\n",
    "    # get low_resolution_img features\n",
    "    low_resolution_feature1 = signal.convolve2d(low_resolution_img, hf1, 'same')\n",
    "    low_resolution_feature2 = signal.convolve2d(low_resolution_img, vf1, 'same')\n",
    "    low_resolution_feature3 = signal.convolve2d(low_resolution_img, hf2, 'same')\n",
    "    low_resolution_feature4 = signal.convolve2d(low_resolution_img, vf2, 'same')\n",
    "\n",
    "    # collect patches from sample\n",
    "    for i in np.arange(num_copies):\n",
    "        row, col = xrow[i], ycol[i]\n",
    "        Hpatch  = high_resolution_img[row:row+patch_size, col:col+patch_size].reshape(-1)\n",
    "        Lpatch1 = low_resolution_feature1[row:row+patch_size, col:col+patch_size].reshape(-1)\n",
    "        Lpatch2 = low_resolution_feature2[row:row+patch_size, col:col+patch_size].reshape(-1)\n",
    "        Lpatch3 = low_resolution_feature3[row:row+patch_size, col:col+patch_size].reshape(-1)\n",
    "        Lpatch4 = low_resolution_feature4[row:row+patch_size, col:col+patch_size].reshape(-1)\n",
    "        Lpatch  = np.concatenate([Lpatch1,Lpatch2,Lpatch3,Lpatch4],axis=0)\n",
    "        x_high[:,i] = Hpatch-np.mean(Hpatch)\n",
    "        x_low[:,i]  = Lpatch\n",
    "\n",
    "    return x_high, x_low\n",
    "\n",
    "def rnd_smp_patch(img_path, type, patch_size, num_patches, upscale):\n",
    "    # get all training images name\n",
    "    img_list = glob.glob(img_path+type) # type = '*.tif'\n",
    "    # get total number of images being considered\n",
    "    img_num = len(img_list)\n",
    "    # initialize number of copies for each image\n",
    "    # depends on its size\n",
    "    num_copies_img = np.zeros(img_num)\n",
    "\n",
    "    # read images and determine number of copies for each image\n",
    "    # this number is proportional to total number of patches\n",
    "    for i in np.arange(img_num):\n",
    "        img = tif.imread(img_list[i])\n",
    "        num_copies_img[i] = np.prod(img.shape)\n",
    "    num_copies_img = np.floor(num_copies_img*num_patches/np.sum(num_copies_img)).astype(np.int)\n",
    "\n",
    "    # initialize output\n",
    "    X_high = []\n",
    "    X_low  = []\n",
    "\n",
    "    for i in np.arange(img_num):\n",
    "        num_copies = num_copies_img[i]\n",
    "        img = tif.imread(img_list[i])\n",
    "        img = normalize_image(img, 256)\n",
    "        x_high, x_low = sample_patches(img, patch_size, num_copies, upscale)\n",
    "        X_high.append(x_high)\n",
    "        X_low.append(x_low)\n",
    "\n",
    "    # assemble a numpy ndarray\n",
    "    X_high = np.concatenate(X_high, axis=1)\n",
    "    X_low  = np.concatenate(X_low, axis=1)\n",
    "\n",
    "    # save data\n",
    "    save_path = 'Training/rnd_patches'+str(patch_size)+'_'+str(upscale)+'_'\n",
    "    savetxt(save_path+'X_high.csv', X_high, delimiter=' ')\n",
    "    savetxt(save_path+'X_low.csv', X_low, delimiter=' ')\n",
    "\n",
    "    return X_high, X_low\n",
    "\n",
    "def patch_pruning(X_high, X_low, threshold):\n",
    "    vars = np.var(X_high, axis=0)\n",
    "    idx  = vars > threshold\n",
    "    X_high = X_high[:,idx]\n",
    "    X_low  = X_low[:,idx]\n",
    "    return X_high, X_low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse_coding.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_FeatureSign(gamma, A, b):\n",
    "    \"\"\"\n",
    "    The detail of the algorithm is described in the following paper:\n",
    "    'Efficient Sparse Coding Algorithms', Honglak Lee, Alexis Battle, Rajat Raina, Andrew Y. Ng,\n",
    "    Advances in Neural Information Processing Systems (NIPS) 19, 2007\n",
    "\n",
    "    minimize 0.5*x.T @ A @ x + b.T @ x + gamma * |x|\n",
    "    \"\"\"\n",
    "    A = A.astype(float)\n",
    "    b = b.astype(float)\n",
    "    x = np.zeros(A.shape[0])\n",
    "    eps = 1e-9\n",
    "    grad = A @ x + b\n",
    "    ii = np.argmax(np.abs(grad*(x==0)))\n",
    "\n",
    "    while True:\n",
    "        if grad[ii] > gamma+eps:\n",
    "            x[ii] = (gamma-grad[ii])/A[ii,ii]\n",
    "        elif grad[ii] < -gamma-eps:\n",
    "            x[ii] = (-gamma-grad[ii])/A[ii,ii]\n",
    "        else:\n",
    "            if np.all(x==0):\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "            # consider active set\n",
    "            activated = x != 0\n",
    "            AA = A[activated,:][:,activated]\n",
    "            bb = b[activated]\n",
    "            xx = x[activated]\n",
    "            # new b based on unchanged sign\n",
    "            # Ax + b + gamma * sign(x) = 0\n",
    "            b_new = -gamma*np.sign(xx)-bb\n",
    "            # analytical solution\n",
    "            x_new = np.linalg.inv(AA)@b_new\n",
    "            idx   = x_new != 0\n",
    "            cost_new  = (b_new[idx]/2 + bb[idx]).T @ x_new[idx] + \\\n",
    "                                gamma*np.sum(np.abs(x_new[idx]))\n",
    "            change_signs = np.where(xx*x_new <= 0)[0]\n",
    "            # if no sign change, x_new is optimum since it's analytical solution\n",
    "            if len(change_signs) == 0:\n",
    "                x[activated] = x_new\n",
    "                loss         = cost_new\n",
    "                break\n",
    "            # find the best interpolation solution x_inter between x_new and xx\n",
    "            # x_inter is improved compared with xx, because of convexity\n",
    "            x_min    = x_new\n",
    "            cost_min = cost_new\n",
    "            d        = x_new-xx\n",
    "            t        = d/xx\n",
    "            for pos in change_signs:\n",
    "                x_inter = xx - d/t[pos] # interpolating at pos-th point\n",
    "                x_inter[pos] = 0        # make sure it is zero\n",
    "                idx = x_inter != 0\n",
    "                # cost of x_inter\n",
    "                cost_temp = (AA[idx,:][:,idx]@x_inter[idx]/2+bb[idx]).T@x_inter[idx] + \\\n",
    "                        gamma*np.sum(np.abs(x_inter[idx]))\n",
    "                if cost_temp < cost_min:\n",
    "                    x_min = x_inter\n",
    "                    cost_min = cost_temp\n",
    "            # update x and loss\n",
    "            x[activated] = x_min\n",
    "            loss         = cost_min\n",
    "\n",
    "        grad  = A @ x + b\n",
    "        ii    = np.argmax(np.abs(grad*(x==0)))\n",
    "        max_x = np.abs(grad[ii])\n",
    "        if max_x <= gamma+eps:\n",
    "            break\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def L1_FeatureSign_Setup(X, D, labda):\n",
    "    \"\"\"\n",
    "    minimize ||X[:,j] - D*Z[:,j]||_2^2 + 2*sigma^2*beta |Z[:,j]|_1\n",
    "    2*sigma^2*beta = labda\n",
    "    since z = Z[:,j] could be separately optimized\n",
    "    \"\"\"\n",
    "    M, N = X.shape\n",
    "    K = D.shape[1]\n",
    "    Z = np.zeros((K,N))\n",
    "    A = D.T @ D\n",
    "    for i in np.arange(N):\n",
    "        b = -D.T @ X[:,i]\n",
    "        Z[:,i] = L1_FeatureSign(labda, A, b)\n",
    "    return Z\n",
    "\n",
    "def object_func(dual_lambda, ZZt, XZt, X, c, trXXt):\n",
    "    # objective function at given dual_lambda\n",
    "    L = XZt.shape[0]\n",
    "    M = len(dual_lambda)\n",
    "    ZZt_inv = np.linalg.inv(ZZt+np.diag(dual_lambda))\n",
    "    if L > M:\n",
    "        f = -np.trace(ZZt_inv @ (XZt.T @ XZt)) + trXXt - c*np.sum(dual_lambda)\n",
    "    else:\n",
    "        f = -np.trace(XZt @ ZZt_inv @ XZt.T) + trXXt - c*np.sum(dual_lambda)\n",
    "    return f\n",
    "\n",
    "def gradient_func(dual_lambda, ZZt, XZt, X, c, trXXt):\n",
    "    L = XZt.shape[0]\n",
    "    M = len(dual_lambda)\n",
    "    ZZt_inv = np.linalg.inv(ZZt+np.diag(dual_lambda))\n",
    "    # gradient of the function at given dual_lambda\n",
    "    g = np.zeros((M,1))\n",
    "    temp = XZt @ ZZt_inv\n",
    "    g = np.sum(temp**2, axis=0)-c\n",
    "    return g\n",
    "\n",
    "def hessian_func(dual_lambda, ZZt, XZt, X, c, trXXt):\n",
    "    L = XZt.shape[0]\n",
    "    M = len(dual_lambda)\n",
    "    ZZt_inv = np.linalg.inv(ZZt+np.diag(dual_lambda))\n",
    "    # Hessian evaluated at given dual_lambda\n",
    "    temp = XZt @ ZZt_inv\n",
    "    h = -2 * (temp.T @ temp) * ZZt_inv\n",
    "    return h\n",
    "\n",
    "def L2_Lagrange_Dual(X, Z, c):\n",
    "    M, N = X.shape\n",
    "    K = Z.shape[0]\n",
    "\n",
    "    ZZt = Z @ Z.T\n",
    "    XZt = X @ Z.T\n",
    "\n",
    "    # arbitrary initialization dual_lambda as Gaussian\n",
    "    dual_lambda = 10*np.abs(np.random.rand(K))\n",
    "    trXXt = np.sum(X**2)\n",
    "    options = {'disp':True, 'maxiter': 100}\n",
    "    # those three are good, if warning says Desired error not necessarily achieved due to precision loss.\n",
    "    # try another method\n",
    "    res = minimize(object_func, dual_lambda, args=(ZZt, XZt, X, c, trXXt), jac = gradient_func, method='CG', options=options)\n",
    "    #res = minimize(object_func, dual_lambda, args=(ZZt, XZt, X, c, trXXt), jac = gradient_func, hess = hessian_func, method='trust-ncg', options=options)\n",
    "    #res = minimize(object_func, dual_lambda, args=(ZZt, XZt, X, c, trXXt), jac = gradient_func, hess = hessian_func, method='Newton-CG', options=options)\n",
    "    dual_lambda = res.x\n",
    "    Dt = np.linalg.inv(ZZt+np.diag(dual_lambda)) @ XZt.T\n",
    "    D  = Dt.T\n",
    "    return D\n",
    "\n",
    "def sparse_coding(X, num_basis, labda, num_iters=50, batch_size=500, initD=None):\n",
    "    \"\"\"\n",
    "    Regularized Sparse Coding\n",
    "    X:          preprocessed np.ndarray, dimension: MxN\n",
    "    num_basis:  number of basis. D.shape[1]\n",
    "    gamma:      sparsity regularization\n",
    "    num_iters:  number of iterations\n",
    "    batch_size: batch size\n",
    "    initD:      initial dictionary\n",
    "\n",
    "    D:          learned dictionary, dimension: MxK, where K << N, K=num_basis\n",
    "    Z:          sparse code, dimension: KxN\n",
    "\n",
    "    this function solve:\n",
    "        minimize_{D,Z} ||X-DZ||_2^2 + lambda*|Z|_1\n",
    "        s.t. column norm of D <= c\n",
    "    This is not convex in both D and Z, but is convex in one of them with the other fixed.\n",
    "    \"\"\"\n",
    "    M, N = X.shape # M: patch_size, N: num_patches\n",
    "    K    = num_basis\n",
    "    X = X.astype(float)\n",
    "    if batch_size == None: batch_size = N\n",
    "    if initD == None:\n",
    "        # Initialize D with a Gaussian random matrix\n",
    "        D = np.random.rand(M, K)-0.5\n",
    "        D = D - np.mean(D, axis=0)\n",
    "        # each column is unit normalized\n",
    "        D = D/np.sqrt(np.sum(D**2, axis=0))\n",
    "    else:\n",
    "        D = initD\n",
    "\n",
    "    # optimization loop:\n",
    "    for iter in np.arange(num_iters):\n",
    "        print('{}-th iterations for sparse coding'.format(iter))\n",
    "        idx = np.arange(N)\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        for i in np.arange(N//batch_size):\n",
    "            batch_idx = idx[np.arange(batch_size)+batch_size*(i-1)]\n",
    "            X_batch   = X[:,batch_idx]\n",
    "\n",
    "            # fix D, update Z\n",
    "            # Z = argmin_Z ||X-DZ||_2^2 + labda*|Z|_1\n",
    "            # for the paper referenced, labda = sigma^2 * beta\n",
    "            # this is provided by Honglak Lee. et al (2017), Efficient sparse coding algorithms\n",
    "            Z = L1_FeatureSign_Setup(X_batch, D, labda)\n",
    "\n",
    "            # fix Z, update D\n",
    "            # c = 1 sum_j ||D_i^j|| \\leq 1, i = 1,2,...,k, column norm constraints\n",
    "            D = L2_Lagrange_Dual(X_batch, Z, 1)\n",
    "\n",
    "    return D, Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_coupled_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_coupled_dict(X_high, X_low, num_basis, labda, upscale):\n",
    "    \"\"\"\n",
    "    solve:\n",
    "    D = argmin ||X-DZ||_2^2 + labda * |Z|_1\n",
    "    s.t ||D_i||_2^2 <= 1, for i = 1, 2, 3, ..., num_basis\n",
    "    \"\"\"\n",
    "    high_dim = X_high.shape[0]\n",
    "    low_dim  = X_low.shape[0]\n",
    "    # should pre-normalize X_high and X_low\n",
    "    high_norm  = np.sqrt(np.sum(X_high**2, axis=0))\n",
    "    low_norm   = np.sqrt(np.sum(X_low**2, axis=0))\n",
    "    nontrivial = np.intersect1d(np.where(high_norm != 0)[0], np.where(low_norm != 0)[0])\n",
    "\n",
    "    X_high = X_high[:,nontrivial]\n",
    "    X_low  = X_low[:,nontrivial]\n",
    "\n",
    "    X_high = X_high/np.sqrt(np.sum(X_high**2, axis=0))\n",
    "    X_low  = X_low/np.sqrt(np.sum(X_low**2, axis=0))\n",
    "\n",
    "    # joint learning of the dictionary\n",
    "    X = np.concatenate([np.sqrt(high_dim)*X_high, np.sqrt(low_dim)*X_low], axis=0)\n",
    "    X_norm = np.sqrt(np.sum(X**2, axis=0))\n",
    "    X = X[:,X_norm > 1e-5]\n",
    "    X = X/np.sqrt(np.sum(X**2, axis=0))\n",
    "\n",
    "    # dictionary training\n",
    "    D, Z = sparse_coding(X, num_basis, labda) # X = DZ, num_basis = D.shape[1]\n",
    "    D_high = D[:high_dim,:]\n",
    "    D_low  = D[high_dim:,:]\n",
    "\n",
    "    # normalize the dictionary\n",
    "    # some column is not useful due to zero.\n",
    "    high_norm  = np.sqrt(np.sum(D_high**2, axis=0))\n",
    "    low_norm   = np.sqrt(np.sum(D_low**2, axis=0))\n",
    "    nontrivial = np.intersect1d(np.where(high_norm != 0)[0], np.where(low_norm != 0)[0])\n",
    "\n",
    "    D_high = D_high[:,nontrivial]\n",
    "    D_low  = D_low[:,nontrivial]\n",
    "\n",
    "    D_high = D_high/np.sqrt(np.sum(D_high**2, axis=0))\n",
    "    D_low  = D_low/np.sqrt(np.sum(D_low**2, axis=0))\n",
    "\n",
    "    return D_high, D_low\n",
    "\n",
    "def train_coupled_dict_before_opt(X_high, X_low, num_basis, labda, upscale):\n",
    "    \"\"\"\n",
    "    Unfortunately, I did not find python equivalent to solve this function\n",
    "    I have to go to MATLAB and carry the result back.\n",
    "    solve:\n",
    "    D = argmin ||X-DZ||_2^2 + labda * |Z|_1\n",
    "    s.t ||D_i||_2^2 <= 1, for i = 1, 2, 3, ..., num_basis\n",
    "    \"\"\"\n",
    "    high_dim = X_high.shape[0]\n",
    "    low_dim  = X_low.shape[0]\n",
    "    # should pre-normalize X_high and X_low\n",
    "    high_norm  = np.sqrt(np.sum(X_high**2, axis=0))\n",
    "    low_norm   = np.sqrt(np.sum(X_low**2, axis=0))\n",
    "    nontrivial = np.intersect1d(np.where(high_norm != 0)[0], np.where(low_norm != 0)[0])\n",
    "\n",
    "    X_high = X_high[:,nontrivial]\n",
    "    X_low  = X_low[:,nontrivial]\n",
    "\n",
    "    X_high = X_high/np.sqrt(np.sum(X_high**2, axis=0))\n",
    "    X_low  = X_low/np.sqrt(np.sum(X_low**2, axis=0))\n",
    "\n",
    "    # joint learning of the dictionary\n",
    "    X = np.concatenate([np.sqrt(high_dim)*X_high, np.sqrt(low_dim)*X_low], axis=0)\n",
    "    X_norm = np.sqrt(np.sum(X**2, axis=0))\n",
    "    X = X[:,X_norm > 1e-5]\n",
    "    X = X/np.sqrt(np.sum(X**2, axis=0))\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def train_coupled_dict_after_opt(D, Z):\n",
    "    # dictionary training\n",
    "    D, Z = sparse_coding(X, num_basis, labda) # X = DZ, num_basis = D.shape[1]\n",
    "    D_high = D[:high_dim,:]\n",
    "    D_low  = D[high_dim:,:]\n",
    "\n",
    "    # normalize the dictionary\n",
    "    # some column is not useful due to zero.\n",
    "    high_norm  = np.sqrt(np.sum(D_high**2, axis=0))\n",
    "    low_norm   = np.sqrt(np.sum(D_low**2, axis=0))\n",
    "    nontrivial = np.intersect1d(np.where(high_norm != 0)[0], np.where(low_norm != 0)[0])\n",
    "\n",
    "    D_high = D_high[:,nontrivial]\n",
    "    D_low  = D_low[:,nontrivial]\n",
    "\n",
    "    D_high = D_high/np.sqrt(np.sum(D_high**2, axis=0))\n",
    "    D_low  = D_low/np.sqrt(np.sum(D_low**2, axis=0))\n",
    "\n",
    "    return D_high, D_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "import tifffile as tif\n",
    "from scipy import signal\n",
    "import cv2\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
