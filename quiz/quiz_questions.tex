\documentclass{article}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{bm}
\usepackage{index}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {./} }
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}
\usepackage{siunitx}
\usepackage{longtable}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
%% ========New command added by Han Liu===========================
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newenvironment{qparts}{\begin{enumerate}[1.]}{\end{enumerate}}
\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate}
\usepackage{subcaption}
\usepackage[figurename=Fig.,labelfont=bf,labelsep=period]{caption}
\usepackage[capitalise]{cleveref}
\usepackage[ruled,vlined]{algorithm2e}
%% ===============================================================

\begin{document}
\title{EECS289A Introduction to Machine Learning,  Project T Final - Quiz Questions}
\author{%
  \ Han Liu, \ Peng Tan, \ Dilu Xu, \ Jinyan Zhao \\
  Department of Civil Engineering\\
  Universitu of California, Berkeley\\
  Berkeley, CA 94704 \\
  \texttt{\{han\_liu, tanpeng, diluxu, jinyan\_zhao\}@berkeley.edu} \\
}
\maketitle
\section{Lagrange Duality and the LASSO}
This question is extra credit!

It is often the case that we will wish to include additional constraints on our regularization terms. For instance if we wish to limit the size of our weights to be below a specific constant term, or if we wish that all of our weights should be non-negative, etc. in these scenarios, Lagrange multipliers are a powerful tool
that allows us to incorporate these constraints into our objective functions.
Also, in general, we have seen for $n \times p$ data, with n observation and p features that $n >> p$ where p is the dominant factor for the computation time in things like linear regression training, which is fine. However, in some scenarios $p >> n$, and this becomes a problem such as when we use the kernel trick! We find that reformulating these problems into their duals gives us the alternative dominant factor.

Here we will explore both of these phenomena simultaneously and see how to formulate a problem with the Lagrangian dual!

This problem follows the formulation in Osborne et al, 2000:

\underline{http://www4.stat.ncsu.edu/~hzhang/paper/osborne.pdf}

By its original definition, the LASSO estimator solves the optimization problem
\begin{equation}
\text{minimize}_{w_1,\cdots, w_m}f(\vec{w})=\frac{1}{2}\sum^n_{i=1}(y_i-\sum^m_{j=1}x_{ij}w_j)^2
\end{equation}
subject to 
\begin{equation}
f_1(\vec{w})=\sum^m_{j=1}|w_j|\leq t\quad\text{for some $t>0$}
\end{equation}
I.e. it minimizes half the sum of squared errors, subject to the constraint that $|\hat{w}|_1\leq t$.

In this question, you will use Lagrangian duality to show that this formulation is equivalent to the following optimization problem (i.e. standard L1-regularized regression):
\begin{equation} \label{eq:dual}
\text{minimize}_{w_1,\cdots, w_m}f(\vec{w})=\frac{1}{2}\sum^n_{i=1}(y_i-\sum^m_{j=1}x_{ij}w_j)^2+\lambda\sum^m_{j=1}|w_j|\quad\text{for some $\lambda>0$}
\end{equation}
Specifically: you will convert the constrained optimization problem 1 into a Lagrange Multiplier problem. From there you will find that when w is an optimal solution to the constrained optimization problem and $\lambda$ is the Lagrange multiplier associated with w, then w is also an optimal solution of the regularized regression problem with penalty coefficient $\lambda$. Your task is to complete part of this proof.
\begin{qparts}
\item {[}5 points{]}
The function 3 is convex in w (quadratic plus L1 norm), which means its minima satisfy
$\triangledown$w(Eq. 3) = 0. Compute the gradient with respect to w of this expression and show that the resulting condition for any optimal w is
\begin{equation}
0=-X^T(y-XW)+\lambda v
\end{equation}

where $v=(v_1,\cdots, v_m)\in\mathbb{R}^m$ satisfies $v_i=\begin{cases}
1 & \text{if $w_i>0$} \\
-1 & \text{if $w_i<0$} \\
\in[-1,1] &\text{if $w_i=0$} 
\end{cases}$
This awkward-seeming vector v is the result of taking the derivative of absolute value terms $|w_i|$, where the wi are of unknown sign. \textit{Hint: this is similar to the derivation of the normal equation in the regression notes.}\\
\underline{\textbf{Answer:}}\\
{\color{blue}
The same as the derivation of the normal equation in the notes; no need to provide
much more detail. The v term is new and arises directly from taking partial derivatives of the $|w_i|$ terms.}

\item {[}5 points{]}
The next step is to take each case of the KKT conditions and solve the constrained optimization problem using Lagrange multipliers. To do this, note that if X is full-rank and $m \leq n$, then the ordinary least-squares regression (OLS) problem has a unique solution:
\begin{equation}
\hat{w}=(X^TX)^{-1}X^TY
\end{equation}
On the other hand, if $m > n$, then the solution $\hat{w}$ to the OLS problem is not unique. Let W be the set of solutions to a particular OLS problem. One can show (see Osbourne et al.) that if $t_0$ is defined as
\begin{equation}
t_0=\min_{\vec{w}\in W}||\vec{w}||_1\quad\text{L1 norm}
\end{equation}
then the constraint 2 is active if and only if $t < t_0$. In other words, if $t \geq t_0$, then the constraint doesn’t matter– a solution to 1 just has to optimize the unconstrained OLS problem. On the other hand, if $t < t_0$, then the constraint 2 must be active for any solution w; in other words, this inequality constraint becomes an equality. Moreover, one can show that when $t < t_0$, at least one optimal w for
the constrained problem 1 is guaranteed to exist on the boundary of the feasible region (the feasible region is the set of all w such that the constraint 2 is satisfied).

Based on this discussion, we only need to find solutions for one case of the KKT conditions when $t < t_0$. Which case is this, and why? \\
\underline{\textbf{Answer:}}\\
{\color{blue}
When t < t0, the constraint must be active at any optimum.
This means we must have $t - f_1(w) = 0$.
}

\item {[}10 points{]}
Define
\begin{equation}
L^*(\vec{w})=\max_{\lambda\geq 0}L(\vec{w}, \lambda)
\end{equation}
Show that
\begin{equation}
L^*(\vec{w})=\begin{cases}
f(\vec{w}) & \text{if $t - f_1(w) \geq 0$} \\
\infty & \text{if $t - f_1(w) < 0$} \\
\end{cases}
\end{equation}
Argue that, therefore, when $t < t_0$, minimizing $L^*(\vec{w})$ over all possible w is equivalent to solving the constrained optimization problem 1. Minimizing $L^*(\vec{w})$ is known as the primal problem.
\textit{Hint: Show that if w optimizes the constrained problem, then it also must minimize $L^*$. Next, show that if w minimizes $L^*$, then it also must optimize the constrained problem. Proof by contradiction may be useful.}\\
\underline{\textbf{Answer:}}\\
{\color{blue}
In the first case: when $t - f_1(w) \geq 0$, then $L(w, \lambda)$ is maximized when $\lambda = 0$. This leaves only $f(w)$. In the second case: when $t - f_1(w) < 0$, then bigger $\lambda$ always makes $L(w, \lambda)$ bigger; hence the max goes to infinity.

Suppose w minimizes the constrained optimization problem. Then from the previous question, we must have $t - f_1(w) = 0$. Suppose there exists $w_0$ such that $L^*(\vec{w}_0) < L^*(\vec{w})$. Since $t - f_1(w) = 0$, we must then have $f(\vec{w}_0) < f(\vec{w})$ with the constraint satisfied. But then w does not minimize the constrained optimization problem; contradiction. On the other hand, suppose w minimizes $L^*$. For any $t \geq 0$, there always exists some w with L1 norm not greater than t. Hence w is in the feasible region $t - f1(w) \geq 0$, and the minimal $L^*(\vec{w})$ is finite. Suppose $f(w0) < f(w) $for feasible $w0$. Then we would also have $L^*(\vec{w}_0) < L^*(\vec{w})$, contradition. This completes the proof of equivalence between the two problems.
}

\item {[}5 points{]}
Define
\begin{equation}
g(\lambda)=\min_{\vec{w}\in\mathbb{R}^m}L(\vec{w}, \lambda)
\end{equation}
Maximizing $g(\lambda)$ for $\lambda\geq 0$ is the dual problem to the primal shown above.

One can show that, for fixed $\lambda$, $L(\vec{w}, \lambda)$ has at least one minimum over w. We can solve for a minimum in the usual way: by taking the gradient of L with respect to w and setting it to zero. Do this to show that if $\bar{w}$ minimizes $L(\vec{w}, \lambda)$ for fixed $\lambda$, then
\begin{equation}
0=-X^T(y-X\bar{w})+\lambda v
\end{equation}
with v defined as before, in 4.\\
\underline{\textbf{Answer:}}\\
{\color{blue}
Same as before; only the extra $-\lambda t$ term is new, and it vanishes when we take the
gradient with respect to $\vec{w}$.
}

\item {[}5 points{]}
Show that $\vec{v}^T\bar{w}=||\bar{w}||_1$. Consequently, show that if $\bar{w}$ minimizes $L(\vec{w}, \lambda)$ for fixed $\Lambda$, then
\begin{equation}
\lambda=\frac{(y-X\bar{w})^TX\bar{w}}{||\bar{w}||_1}
\end{equation}
We call this value of $\lambda$ the Lagrange multiplier associated with $\bar{w}$. \textit{Hint: recall that $(AB)^T=B^TA^T$.}\\
\underline{\textbf{Answer:}}\\
{\color{blue}
First, $\vec{v}^T\bar{w}=\sum^m_{i=1}v_i\bar{w}_i$. By definition, each $v_i$ has magnitude 1 and the same sign as $\bar{w}_i$. Hence their product is $|\bar{w}_i|$. The sum of these is the L1 norm of $\vec{w}$.

Next, we start with

\begin{gather}
0=-X^T(y-X\bar{w})+\lambda v, \quad\text{and so} \\
\lambda v=X^T(y-X\bar{w}), \quad\text{and so} \\
\lambda v^T=(y-X\bar{w})^TX, \quad\text{and so} \\
\lambda v^T\bar{w}=(y-X\bar{w})^TX\bar{w}, \quad\text{and so} \\
\lambda=\frac{(y-X\bar{w})^TX\bar{w}}{||\bar{w}||_1}\quad\text{as desired.}
\end{gather}
It turns out that strong duality holds for this problem (see Theorem 3 in Osborne et al), which means: if $w^*$ is a solution of the primal problem (equivalently, of the constrained optimization problem 1) and $\lambda^*$ is the Lagrange multiplier corresponding to $w^*$, then $\lambda^*$ is a solution of the dual problem, and hence
the optimal primal and dual function values are equal:
\begin{equation}
g(\lambda^*)=L^*(\vec{w}^*)=f(\vec{w}^*)
\end{equation}
for f as defined in 1.

Let’s review: we began by trying to solve the constrained optimization problem 1. We saw that this is equivalent to solving the primal problem 7. Due to strong duality, we can solve either the primal or dual problem to obtain an optimal w for 1. Given a solution w for 1, we can compute the Lagrange multiplier associated with it using 11. On the other hand, given $\lambda$, any optimal w for the penalized regression problem 3 satisfies exactly the same equations (4) as are satisfied by solutions to 1. This means: when w is an optimal solution to the constrained optimization problem and $\lambda$ is the Lagrange
multiplier associated with w, then w is also an optimal solution of the penalized regression problem with penalty coefficient $\lambda$.
}


\end{qparts}
\newpage
\section{Basic Question}
\begin{qparts}
\item  A computer program is said to learn from experience E with respect to some task T and some performance measure P if its performance on T, as measured by P, improves with experience E.\\
 Suppose we feed a learning algorithm a lot of historical weather data, and have it learn to predict weather. What would be a reasonable choice for P? \\
a. The probability of it correctly predicting a future date’s weather.\\
b. The weather prediction task.\\
c. The process of the algorithm examining a large amount of historical weather data.\\
d. None of these.\\
\underline{\textbf{Answer:}}\\
a


 \item  A computer program is said to learn from experience E with respect to some task T and some performance measure P if its performance on T, as measured by P, improves with experience E. \\
 Suppose we feed a learning algorithm a lot of historical weather data, and have it learn to predict weather. In this setting, what is T?\\
 a. The weather prediction task.\\
b. The probability of it correctly predicting a future date’s weather.\\
c. The process of the algorithm examining a large amount of historical weather data.\\
d. None of these.\\
 
\underline{\textbf{Answer:}}\\
a

 \item  Suppose you are working on weather prediction, and use a learning algorithm to predict tomorrow’s temperature (in degrees Centigrade/Fahrenheit).\\
Would you treat this as a classification or a regression problem?\\
a. Regression\\
b. Classification\\
 
\underline{\textbf{Answer:}}\\
a

 \item  Suppose you are working on weather prediction, and your weather station makes one of three predictions for each day’s weather: Sunny, Cloudy or Rainy. You’d like to use a learning algorithm to predict tomorrow’s weather.\\
Would you treat this as a classification or a regression problem?\\
a. Regression\\
b. Classification\\
 
\underline{\textbf{Answer:}}\\
b

\end{qparts}


\newpage
\section{Elastic Net}
In class we discussed two types of regularization, $l_1$ and $l_2$. Both are useful, and sometimes it is helpful combine them, giving the objective function below (\textbf{in this problem we have excluded} $w_0$ \textbf{for simplicity}):
\begin{equation}
F(\vec{w}) = \frac{1}{2}\sum^n_{j=1}(y^{(j)}-\sum^d_{i=1}\vec{w}_i\vec{x}_i^{(j)})^2+\alpha\sum^d_{i=1}|\vec{w}_i|+\frac{\lambda}{2}+\sum^d_{i=1}\vec{w}_i^2
\end{equation}
Here, $(\vec{x}^{(j)},y^{(j)})$ is j-th example in the training data, w is a d dimensional weight vector, $\lambda$ is a regularization parameter for the $l_2$ norm of w, and $\alpha$ is a regularization parameter for the $l_1$ norm of w. This approach is called the Elastic Net, and you can see that it is a generalization of Ridge and Lasso regression: It reverts to Lasso when $\lambda = 0$, and it reverts to Ridge when $\alpha = 0$. In this question, we are going to derive the coordinate descent (CD) update rule for this objective.

Let g, h, c be real constants, and consider the function of x
\begin{equation}
f_1(x) = c+gx+\frac{1}{2}hx^2
\end{equation}
\begin{qparts}
\item {[}4 points{]} 
What is the $x^*$ that minimizes $f_1(x)$? (i.e. calculate $x^*=\arg\min f_1(x)$)\\
\underline{\textbf{Answer:}}\\
Take the gradient of $f_1(x)$ and set it to 0.
\begin{alignat}{2}
g+hx&=0 \\
x^*&=-\frac{g}{h}
\end{alignat}   

Let $\alpha$ be an additional real constant, and consider another function of x
\begin{equation}\label{eq:hl1}
f_2(x) = c+gx+\frac{1}{2}hx^2+\alpha|x|(h>0, \alpha>0)
\end{equation}
This is a piecewise function, composed of two quadratic functions:
\begin{equation}
f_2^-(x) = c+gx+\frac{1}{2}hx^2-\alpha x
\end{equation}
and
\begin{equation}
f_2^+(x) = c+gx+\frac{1}{2}hx^2+\alpha x
\end{equation}
Let $\tilde{x}^-=\arg\min_{x\in\mathbb{R}}f_2^-(x)$ and $\tilde{x}^+=\arg\min_{x\in\mathbb{R}}f_2^+(x)$.\\
(\textbf{Note:} The argmin is taken over $(-\infty, +\infty)$ here.
\\

\item {[}6 points{]} 
What are $\tilde{x}^+$ and $\tilde{x}^+$? Show that $\tilde{x}^-\geq\tilde{x}^+$. \\
\underline{\textbf{Answer:}}\\
Using the answer from part 1), we get $\tilde{x}^+=-\frac{g+\alpha}{h}$ and $\tilde{x}^-=-\frac{g-\alpha}{h}$. Since $\tilde{x}^--\tilde{x}^+=\frac{2\alpha}{h}\geq 0$, we have $\tilde{x}^-\geq\tilde{x}^+$.


\item {[}12 points{]} 
Draw a picture of $f_2(x)$ in each of the three cases below:
\begin{enumerate}[(a)]
\item $\tilde{x}^->0$, $\tilde{x}^+>0$
\item $\tilde{x}^-<0$, $\tilde{x}^+<0$
\item $\tilde{x}^->0$, $\tilde{x}^+<0$
\end{enumerate}
For each case, mark the minimum as either 0, $\tilde{x}^-$, or $\tilde{x}^+$. (You do not need to draw perfect curves, just get the rough shape and the relative locations of the minima to the x-axis)\\
\underline{\textbf{Answer:}}\\
\cref{fig:hl1} gives the example picture of three cases. To understand the answer, note the following:
\begin{itemize}
\item $f^+_2(0)=f^-_2(0)$, this means the two piece of $f_2$ match each other at x = 0.
\item When $\tilde{x}^-\geq 0$, the minimum of the left part is at 0, i.e. $\arg\min_{x\in \left(-\infty, 0\right]}f_2^-(x)=0$
\item When $\tilde{x}^-\leq 0$, the minimum of the left part is at $\tilde{x}^-$, i.e. $\arg\min_{x\in \left(-\infty, 0\right]}f_2^-(x)=\tilde{x}^-$
\item Similar rules holds for the right part of the curve.
\end{itemize}
\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[height=1.3in]{fig/fig-han-1.PNG}
        \caption{$\tilde{x}^->0$, $\tilde{x}^+>0$, the minima is at $\tilde{x}^+$.}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[height=1.3in]{fig/fig-han-2.PNG}
        \caption{$\tilde{x}^-<0$, $\tilde{x}^+<0$, the minima is at $\tilde{x}^-$.}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[height=1.3in]{fig/fig-han-3.PNG}
        \caption{$\tilde{x}^->0$, $\tilde{x}^+<0$, the minima is at 0.}
    \end{subfigure}
    \caption{Example picture of three cases.}
    \label{fig:hl1} 
\end{figure*}
\end{qparts}


\newpage
\section{Elastic Net Plus}
\begin{qparts}
\item {[}8 points{]} 
Write $x^*$, the minimum of $f_2(x)$, as a piecewise function of g. (Hint: what is
g in the cases above?)\\
\underline{\textbf{Answer:}}\\
Summarizing the result from the previous question, we have
\begin{equation}
x^*=
\begin{cases}
\tilde{x}^+ &\text{if $\tilde{x}^+>0$}\\
\tilde{x}^- &\text{if $\tilde{x}^-<0$}\\
0 &\text{if $\tilde{x}^+<0, \tilde{x}^->0$}
\end{cases}
\end{equation}
This is equivalent to the soft-threshold function
\begin{equation}
x^*=
\begin{cases}
   -\frac{g+\alpha}{h} &\text{if $g<-\alpha$}\\
   0 &\text{if $g\in\left[-\alpha, \alpha\right]$}\\
   -\frac{g-\alpha}{h} &\text{if $g>\alpha$}
   \end{cases}
\end{equation}

\item {[}8 points{]} 
Now let’s derive the update rule for $w_k$. Fixing all parameters but $w_k$, express the objective function in the form of Eq.~\eqref{eq:hl1} (here $w_k$ is our x). You do not
need to explicity write constant factors, you can put them all together as c. What are
g and h? Then putting it all together, write down the coordinate descent algorithm for the Elastic Net (\textbf{again excluding} $w_0$). You can write the update for $w_k$ in terms of the g and h you calculated in the previous question.\\
\underline{\textbf{Answer:}}\\
\begin{equation}
g=\sum^n_{j=1}x^{(j)}_k(\sum_{i\neq k}\vec{w}_i\vec{x}_i^{(j)}-y_i)
\end{equation}
\begin{equation}
h=\lambda+\sum^n_{j=1}(x^{(j)})^2
\end{equation}
The algorithm is shown in Algorithm.~\ref{algo:hl1}. You can compare it to the CD algorithm for Lasso, the only difference is adding $\lambda$ to h.\\
\begin{algorithm}[H]\label{algo:hl1}
\SetAlgoLined
% \KwResult{Write here the result }
%  initialization\;
 \While{not converged}{
  % instructions\;
  \For{$k\in \{1,2,\cdots,d\}$}{
   $g=\sum^n_{j=1}x^{(j)}_k(\sum_{i\neq k}\vec{w}_i\vec{x}_i^{(j)}-y_i)$\;
   $h=\lambda+\sum^n_{j=1}(x^{(j)})^2$\;
   $w_k=\begin{cases}
   -\frac{g+\alpha}{h} &\text{if $g<-\alpha$}\\
   0 &\text{if $g\in\left[-\alpha, \alpha\right]$}\\
   -\frac{g-\alpha}{h} &\text{if $g>\alpha$}
   \end{cases}$\;
   }{
   % instructions3\;
  }
 }
 \caption{Coordinate Descent for Elastic Net}
\end{algorithm}
\end{qparts}

\newpage
\section{True or False questions}
Answer True or False. Justify your answer very briefly in 1-2 sentences.
\begin{qparts}
\item Increasing the regularization parameter $\lambda$ in lasso regression leads to sparser regression coefficients.
\underline{\textbf{Answer:}}\\
True. Larger regularization parameter penalizes non-zero coefficients more, leading to
sparser solution.

\item In Lasso regression, can the regulariser increase the sparsity of the resulting solutions?\\
\underline{\textbf{Answer:}}\\
True, Lasso introduces the L1 norm penalty, so increasing the penalty will cause more and more of the parameters to be driven to zero. As seen in \cref{fig:hl2}, where the red contours are representing the error function and the solid blue areas the constraint regions for the parameters, in the Lasso case, the function is likely to hit the constraint region on one of the four corners. This will make a parameter equal with zero.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=.6\textwidth]{fig/fig-han-4.PNG}
    \caption{Spam-Estimation picture for the LASSO (left) and Ridge regression (right).}
    \label{fig:hl2}
\end{figure}

\item Is the objective of Lasso regression differentiable and convex?\\
\underline{\textbf{Answer:}}\\
False, it is not differentiable. As Lasso uses L1 norm penalty, this L1 loss function is defined based on the absolute value of the difference between two values. Since the loss function is a modulus function, there will exist an inflection point where the slope cannot be computed and the function is not differentiable. In contrast, for the case of Ridge regression, where we use L2 norm penalty, we can get the slope at any point and the objective is differentiable.

\item Are the objectives of Ridge and Lasso regression both convex and have closed-form solutions?\\
\underline{\textbf{Answer:}}\\
False, they are both convex, but Lasso does not always have a closed-form solution. LASSO regression must be solved numerically by using quadratic programming or more general convex optimisation methods.

\item Is it true that the L1 term in Lasso has the following purposes: performing feature selection, compensating for overfitting and smoothing?\\
\underline{\textbf{Answer:}}\\
False. The first two purposes are true, but not the last one. As it is mentioned early, both L1 and L2 terms are introduced to compensate for overfitting. By making the parameters equal with zero, the L1 norm discards some features and, thus, performs feature selection. In terms of smoothing, we cannot say L1 has this purpose, as the L1 loss function does not have continuous derivatives.
\end{qparts}


\newpage
\section{Quick questions}
Explain \textbf{in one or two sentences} why the statements are true (or false).
\begin{qparts}
\item L2 loss is more robust to outliers than L1 loss.\\
\underline{\textbf{Answer:}}\\
False \\
The gradient of L2 loss can grow without bound whereas the L1 loss gradient is bounded, hence the influence of an outlier is limited.

\item Logistic loss is better than L2 loss in classification tasks.\\
\underline{\textbf{Answer:}}\\
True \\ 
With logistic loss, correctly classified points that are far away from the decision boundary have much less impact on the decision boundary.


\item In terms of feature selection, L2 regularization is preferred since it comes up with sparse solutions.\\
\underline{\textbf{Answer:}}\\
False \\
L1 regularization (LASSO) comes up with sparse solutions due to nonvanishing gradient at 0.
\end{qparts}


\newpage
\section{Gradient descent}
Denote by $\vec{x}\in\mathbb{R}^d$ data, by $\vec{w}\in\mathbb{R}^d$ the weight vector, by $y\in\mathbb{R}y$ lables, by $\lambda>0$ a regularization constant, and by m the total number of data points. Let R(w) be the regularized risk function:
\begin{equation}
R(w):=\frac{1}{m}\sum^m_{i=1}l(y_i-\left<w,x_i\right>)+\frac{\lambda}{2}||\vec{w}||^2 \quad\text{where}\quad l(\xi)=\begin{cases}
\frac{1}{2}\xi^2 &\text{if $|\xi|<1$} \\
|\xi|-\frac{1}{2} &\text{otherwise}
\end{cases}
\end{equation}
\begin{qparts}
\item Calculate the batch gradient of R(w) with respect to w.\\
\underline{\textbf{Answer:}}\\
Define $g_i$ as the gradient of ith data point.
\begin{equation}
g_i=\begin{cases}
(\left<w,x_i\right>-y)x_i &\text{if$|\left<w,x_i\right>-y|<1$} \\
\text{sgn}(\left<w,x_i\right>-y)x_i &\text{otherwise}
\end{cases}
\end{equation}
\begin{equation}
\frac{\partial R}{\partial w}=\frac{1}{m}\sum^m_{i=1}g_i+\lambda w
\end{equation}

\item Write out a stochastic gradient descent learning algorithm for minimizing R(w).\\
\underline{\textbf{Answer:}}\\
Randomly select a training instance $x_i$, update w as follows:
\begin{equation}
\vec{w}\leftarrow (1-\lambda\eta_t)\vec{w}-\eta_tg_i
\end{equation}

\item Name two common choices for choosing the learning rate and briefly explain their properties.\\
\underline{\textbf{Answer:}}\\
$O(\frac{1}{\sqrt{t}}), O(\frac{1}{t})$ for strong convexity, Polynomial decay, AdaGrad, etc.
\end{qparts}

\newpage
\section{Logistic Regression}

\begin{qparts}
 \item For logistic regression, the gradient is given by $\frac{\partial }{\partial \theta_j } J(\theta) = \frac{1}{m} \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{i})x^{(i)}_j$. Which of these is a correct gradient descent update for logistic regression with a learning rate of $\alpha$? \\
 Check all that apply.\\
 a. $\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)-y^i}) x^{(i)}_j$ (simultaneously update for all j).\\
 b. $\theta := \theta - \alpha \frac{1}{m} \sum_{i=1}^m (\theta^Tx-y^{(i)}) x^{(i)}$.\\
 c. $\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m \left(\frac{1}{1+e^{-\theta^Tx^{(i)}}}-y^{(i)}\right) x^{(i)}_j$ (simultaneously update for all j).\\
 d. $\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)-y^i}) x^{(i)}$ (simultaneously update for all j).\\
 
\underline{\textbf{Answer:}}\\
ac

 \item Suppose that you have trained a logistic regression classifier, and it outputs on a new example a prediction $h_\theta(x) = 0.2$. This means (check all that apply):\\
a. Our estimate for $P(y = 1|x; \theta)$ is 0.8.\\
b. Our estimate for $P(y = 0|x; \theta)$ is 0.8.\\
c. Our estimate for $P(y = 1|x; \theta)$ is 0.2.\\
d. Our estimate for $P(y = 0|x; \theta)$ is 0.2.\\
 
\underline{\textbf{Answer:}}\\
bc

\end{qparts}

\newpage
\section{Multiple choice I}
In this part, you only need to choose one correct answer.
\begin{qparts}
\item You run gradient descent for 15 iterations with $\alpha = 0.3$ and compute after each iteration. You find that the value of $J(\theta)$ decreases slowly and is still decreasing after 15 iterations. Based on this, which of the following conclusions seems most plausible?\\
 a. Rather than use the current value of  $\alpha$, it’d be more promising to try a larger value of $\alpha$ (say $\alpha = 1.0$).\\
 b. Rather than use the current value of $\alpha$, it’d be more promising to try a smaller value of $\alpha$ (say $\alpha = 0.1$).\\
 c. $\alpha = 0.3$ is an effective choice of learning rate.\\
\underline{\textbf{Answer:}}\\
a
\item You run gradient descent for 15 iterations with $\alpha = 0.3$ and compute after each iteration. You find that the value of $J(\theta)$ decreases quickly then levels off. Based on this, which of the following conclusions seems most plausible?\\
 a. Rather than use the current value of  $\alpha$, it’d be more promising to try a larger value of $\alpha$ (say $\alpha = 1.0$).\\
 b. Rather than use the current value of $\alpha$, it’d be more promising to try a smaller value of $\alpha$ (say $\alpha = 0.1$).\\
 c. $\alpha = 0.3$ is an effective choice of learning rate.\\
\underline{\textbf{Answer:}}\\
c

\item Suppose you have $m = 23$ training examples with $n = 5$ features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is $\theta = (X^{T} X)^{-1}X^{T}y$. For the given values of m and n, what are the dimensions of $\theta$, X, and y in this equation?\\
a. X is 23 × 5, y is 23 × 1, $\theta$ is 5 × 5\\
b. X is 23 × 6, y is 23 × 6, $\theta$ is 6 × 6\\
c. X is 23 × 6, y is 23 × 1, $\theta$ is 6 × 1\\
d. X is 23 × 5, y is 23 × 1, $\theta$ is 5 × 1\\
\underline{\textbf{Answer:}}\\
c\\
 X has m rows and n+1 columns (+1 because of the $x_0=1$ term). y is m-vector. $\theta$ is an $(n+1)$-vector


\item Suppose you have a dataset with $m = 1000000$ examples and $n = 200000$ features for each example. You want to use multivariate linear regression to fit the parameters $\theta$ to our data. Should you prefer gradient descent or the normal equation?\\
a. Gradient descent, since it will always converge to the optimal $\theta$.\\
b. Gradient descent, since $(X^T X)^{-1} $will be very slow to compute in the normal equation.\\
c. The normal equation, since it provides an efficient way to directly find the solution.\\
d. The normal equation, since gradient descent might be unable to find the optimal $\theta$.\\
\underline{\textbf{Answer:}}\\
b\\
With $n = 200000$ features, you will have to invert a $200001 x 200001$ matrix to compute the normal equation. Inverting such a large matrix is computationally expensive, so gradient descent is a good choice.
 
 \item Which of the following are reasons for using feature scaling?\\
a. It is necessary to prevent gradient descent from getting stuck in local optima.\\
b. It speeds up solving for $\theta$ using the normal equation.\\
c. It prevents the matrix $X^T X$ (used in the normal equation) from being non-invertable (singular/degenerate).\\
d. It speeds up gradient descent by making it require fewer iterations to get to a good solution.\\
\underline{\textbf{Answer:}}\\
d\\
For a, the cost function $J(\theta)$ for linear regression has no local optima. For b, the magnitute of the feature values are significant in terms of computational cost. For c, feature scaling has nothing to do with matrix inversion. For d, feature scaling speeds up gradient descent by avoiding many extra iterations that are required when one or more features takes on much larger values than he rest.
 
\end{qparts}


\newpage
\section{Multiple choice II}
In this part, some questions may have multiple correct answers.
\begin{qparts}

 \item Some of the problems below are best addressed using a supervised learning algorithm, and the others with an unsupervised learning algorithm. Which of the following would you apply supervised learning to? (Select all that apply.) In each case, assume some appropriate dataset is available for your algorithm to learn from.\\
 a. Given historical data of children’s ages and heights, predict children’s height as a function of their age.\\
 b. Given 50 articles written by male authors, and 50 articles written by female authors, learn to predict the gender of a new manuscript’s author (when the identity of this author is unknown).\\
 c. Take a collection of 1000 essays written on the US Economy, and find a way to automatically group these essays into a small number of groups of essays that are somehow “similar” or “related”.\\
 d. Examine a large collection of emails that are known to be spam email, to discover if there are sub-types of spam mail.\\
 
\underline{\textbf{Answer:}}\\
ab

 \item Some of the problems below are best addressed using a supervised learning algorithm, and the others with an unsupervised learning algorithm. Which of the following would you apply supervised learning to? (Select all that apply.) In each case, assume some appropriate dataset is available for your algorithm to learn from.\\
a. Given data on how 1000 medical patients respond to an experimental drug (such as effectiveness of the treatment, side effects, etc.), discover whether there are different categories or “types” of patients in terms of how they respond to the drug, and if so what these categories are.\\
b. Given a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we might tailor separate treatments.\\
c. Have a computer examine an audio clip of a piece of music, and classify whether or not there are vocals (i.e., a human voice singing) in that audio clip, or if it is a clip of only musical instruments (and no vocals).\\
d. Given genetic (DNA) data from a person, predict the odds of him/her developing diabetes over the next 10 years.\\
 
 \underline{\textbf{Answer:}}\\
cd


 \item Some of the problems below are best addressed using a supervised learning algorithm, and the others with an unsupervised learning algorithm. Which of the following would you apply supervised learning to? (Select all that apply.) In each case, assume some appropriate dataset is available for your algorithm to learn from.\\
a. Take a collection of 1000 essays written on the US Economy, and find a way to automatically group these essays into a small number of groups of essays that are somehow “similar” or “related”.\\
b. Given genetic (DNA) data from a person, predict the odds of him/her developing diabetes over the next 10 years.\\
c. Examine a large collection of emails that are known to be spam email, to discover if there are sub-types of spam mail.\\
d. Examine the statistics of two football teams, and predict which team will win tomorrow’s match (given historical data of teams’ wins/losses to learn from).\\
 
 \underline{\textbf{Answer:}}\\
bc


 \item Which of these is a reasonable definition of machine learning?\\
a. Machine learning is the science of programming computers.\\
b. Machine learning learns from labeled data.\\
c. Machine learning is the field of allowing robots to act intelligently.\\
d. Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.\\
 
 \underline{\textbf{Answer:}}\\
d

\end{qparts}

\newpage
\section{Extra Question}
\begin{qparts}

 \item Which of the following statements about regularization are true? Check all that apply.\\
a. Using a very large value of $\lambda$ hurt the performance of your hypothesis; the only reason we do not set $\lambda$ to be too large is to avoid numerical problems.\\
b. Because logistic regression outputs values$ 0 \leq h_\theta(x) \leq 1$, its range of output values can only be “shrunk” slightly by regularization anyway, so regularization is generally not helpful for it.\\
c. Consider a classification problem. Adding regularization may cause your classifier to incorrectly classify some training examples (which it had correctly classified when not using regularization, i.e. when $\lambda$ = 0).\\
d. Using too large a value of $\lambda$ can cause your hypothesis to overfit the data; this can be avoided by reducing $\lambda$.\\
 \underline{\textbf{Answer:}}\\
 c

 \item Which of the following statements about regularization are true? Check all that apply.\\
a. Using a very large value of $\lambda$ hurt the performance of your hypothesis; the only reason we do not set $\lambda$ to be too large is to avoid numerical problems.\\
b. Because logistic regression outputs values$ 0 \leq h_\theta(x) \leq 1$, its range of output values can only be “shrunk” slightly by regularization anyway, so regularization is generally not helpful for it.\\
c. Because regularization causes $J(\theta)$ to no longer be convex, gradient descent may\\
d. Using too large a value of $\lambda$ can cause your hypothesis to underfit the data; this can be avoided by reducing $\lambda$.\\
 \underline{\textbf{Answer:}}\\
d


\end{qparts}
\end{document}